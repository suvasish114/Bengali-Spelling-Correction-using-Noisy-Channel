# -*- coding: utf-8 -*-
"""Bengali Spelling Correction Using Noisy Channel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kKuL3rwvjRUX4rzoimZcrQlpicRPZci6

# Bengali Spelling Correction Using Noisy Chennel
"""

# Downloading kenlm modules
!pip install https://github.com/kpu/kenlm/archive/master.zip

# PATHS
bn_characters_path  = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_characters.txt"
bn_alphabets_path   = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_alphabets.txt"
bn_digits_path      = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_digits.txt"
bn_sentences_path   = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_sentences_15204216.txt"
# bn_dictionary_path  = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_dictionary_1127183.txt"
bn_dictionary_path  = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_dictionary_5456082.txt"
bn_arpa_path    = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/KENLM_2gram.arpa"   # mine
# bn_arpa_path    = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/large_model_50_correct.arpa"      # by PR sir
bn_del_path     = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_del.txt"
bn_ins_path     = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_ins.txt"
bn_sub_path     = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/bn_sub.txt"
letter_count_b  = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/letter_count_b.txt"
letter_count_u  = "/content/drive/MyDrive/Spelling_Correction_Using_Noisy_Channel_Resources/bangla_datasets/letter_count_u.txt"

# IMPORTS
import time
import numpy as np
import nltk
import kenlm
from nltk.util import ngrams
from collections import Counter

# KENLM MODEL OBJECT
kenlm_model = kenlm.Model(bn_arpa_path)
print(f"order of the arpa file: {kenlm_model.order}")

# REQUIRED LIBRARY (one time)
nltk.download("punkt")

"""## 1. Generate Candidate Words"""

# GET ALPHABETS
with open(bn_alphabets_path, "r") as file:
    bn_alphabets = file.read().split("\t")
print(bn_alphabets)

print(len(bn_alphabets))

# VALUE INDEX PAIR OF CHARACTERS
char_lookup = dict(zip(bn_alphabets, list(range(len(bn_alphabets)))))
char_lookup.update({"#": len(char_lookup)})

# TEST
char_lookup["ই"]

char_lookup["#"]    # hash denoes whitespace

# READ DICTIONARY
with open(bn_dictionary_path, "r") as file:
    bn_dictionary = file.read().split("\n")

# TEST
bn_dictionary[:10]


# FIND ALL CANDIDATES
def find_candidate_words(word):                     # get candidate words with single transformation
    '''----
Description: Find all candidate words for given misspelled word by using {insetion, deletion, transpose, substitution}
Args:   word (string)   = misspelled word
Return: (list)          = a list of all candidate words '''

    candidate_words = []
    for char in bn_alphabets:
        for idx in range(len(word)+1):              # Insert candidate
            candidate_words.append(word[:idx]+char+word[idx:])
        for idx in range(len(word)):                # Substitution candidate
            candidate_words.append(word[:idx]+char+word[idx+1:])
    for idx in range(len(word)):                    # Deletion candidate
        candidate_words.append(word[:idx]+word[idx+1:])
    if(len(word)>1):                                # Transpose candidate
        for idx in range(len(word)-1):
            candidate_words.append(word[:idx]+word[idx+1]+word[idx]+word[idx+2:])
    return candidate_words

"জন্য" in find_candidate_words("জন")

# FIND VALID CANDIDATES
def find_valid_candidates(word, med):    # single edit distance candidates
    '''----
Description: Fina all candidates within given edit distance
Args:   word (string)   = error word
        med (int)       = minimum edit distance within which we want to find our candidates
Return: (list)          = list of all valid candidate words '''

    candidates = []
    candidate_words = find_candidate_words(word)    # for edit distance 1 (initial)
    for i in range(med-1):                          # for edit distance > 1
        for candidate_word in candidate_words:
            candidate_words = set(find_candidate_words(candidate_word)).union(candidate_words)
    valid_candidates = list(set(candidate_words).intersection(set(bn_dictionary))) # match all generated candidate words with dictionary
    return valid_candidates

# test
find_valid_candidates("পূর্বাভা", 1)

"""## 2. Minimum Edit Distance"""

def get_minimum_edit_distance(s1, s2):
    '''----
Descrioption: Calculate the minimum edit distance between two strings. Know more from https://web.stanford.edu/class/cs124/lec/med.pdf
Args:   s1 (string)     = first string; error word
        s2 (string)     = second string; candidate word
Return: (int)           = minimum number of edits requires to transform from s1 to s2,
        (2D numpy array)= calculated cost matrix in tabulated format '''

    m = len(s1)                         # length of source string
    n = len(s2)                         # length of target string
    mat = np.zeros((m+1, n+1))          # 2D array of dimention (n+1 X m+1) filled with 0's

    for i in range(1, m+1):             # Initializing row 1
        mat[i, 0] = mat[i-1, 0] + 1
    for j in range(1, n+1):             # Initializing column 1
        mat[0, j] = mat[0, j-1] + 1

    # Iteration
    for i in range(1, m+1):
        for j in range(1, n+1):
            sc = 2 if s1[i-1] != s2[j-1] else 0
            cost = min([mat[i-1, j] + 1,        # deletion
                        mat[i, j-1] + 1,        # insertion
                        mat[i-1, j-1] + sc])    # substitution
            mat[i, j] = cost                    # update cost

    return int(mat[-1,-1]), mat

def get_edit_sequence(s, d):
    '''----
Description: Calculate edit sequence from the given correct word and typo
Args:   s (string)  = typo (error word) or source word
        d (string)  = candidate or destination word
Return: (list)      = sequence of transformation in reversed order <transformation> <source> <destination> '''

    _, m = get_minimum_edit_distance(s, d)
    s = '#' + s
    d = '#' + d
    sequence = list()
    i, j = len(s)-1, len(d)-1
    while True:
        if i>0 and j>0:     # diagonal element present
            if m[i,j] == m[i-1,j-1] and s[i] == d[j]:
                sequence.append('')
                i -= 1
                j -= 1
            elif m[i,j] == m[i-1,j-1]+2 and s[i] != d[j]:
                sequence.append('s'+d[j]+s[i])
                i -= 1
                j -= 1
            elif m[i,j] == m[i-1,j]+1:
                sequence.append('d'+d[j]+s[i])
                i -= 1
            else:
                sequence.append('i'+d[j]+s[i])
                j -= 1
        elif i==0 and j==0:
            break
        elif i == 0:
            sequence.append('i'+d[j]+s[i])
            j -= 1
        elif j == 0:
            sequence.append('d'+d[j]+s[i])
            i -= 1
    return sequence # [<type>,<destination>,<source>]

# TEST
s = "কলকাতা"
d = "কলিকাতা"
get_edit_sequence(s, d)

"""## 3. Confusion Matrix"""

# Confusion matrix for insersion
bn_ins = list()
with open(bn_ins_path, "r") as file:
    temp = file.read().split("\n")
    for t in temp:
        bn_ins.append(t.strip().split(" "))

# Confusion matrix for deletion
bn_del = list()
with open(bn_del_path, "r") as file:
    temp = file.read().split("\n")
    for t in temp:
        bn_del.append(t.strip().split(" "))

# Confusion matrix for subtitution
bn_sub = list()
with open(bn_sub_path, "r") as file:
    temp = file.read().split("\n")
    for t in temp:
        bn_sub.append(t.strip().split(" "))

letter_b = list()
with open(letter_count_b, "r") as file:
    temp = file.read().split("\n")
    for t in temp:
        letter_b.append(t.strip().split("\t"))

letter_u = list()
with open(letter_count_u, "r") as file:
    letter_u += file.read().split("\n")

def get_bi_letter_count(a, b):
    '''----
Description: return frequency of letter (i.e. freq of 'ab' together) in the corpus
Args:   a (char)    = first character
        b (char)    = second character
Return: (int)       = frequency of the letter together in the corpus '''

    if len(a) != len(b) != 1:
        raise "abort! a!=b!=1"
    return letter_b[char_lookup[a]][char_lookup[b]]

# test
get_bi_letter_count("ক", "ল")

def get_uni_letter_count(a):
    '''----
Description: return frequency of letter (i.e. freq of 'a') in the corpus
Args:   a (char)    = character whose frequency to be determined
Return: (int)       = frequency of the letter in the corpus '''

    if len(a) != 1:
        raise "abort! a!=1"
    return letter_u[char_lookup[a]]

# test
get_uni_letter_count("ক")

"""## 4. Prior Probability"""

# REMOVE PUNCHUATIONS
import string
strip_chars = string.punctuation
def remove_punc(sentence):
    '''----
Descriptin: remove punchuation from a given sentence
Args:   sentence (string)   = sentence from which punchuation needs to be removed
Return: (string)            = sentence with punchuation removed'''

    return sentence.translate(str.maketrans('', '', string.punctuation)).lower()

# RETURNS UNIGRAM SCORE
def get_score(s):
    '''----
Description: find unigram score of tokens from a sentence
Args:   s (string)  = source sentence for which score needs to be generated
Return: (list)      = list of unigram probabilities of tokens from sentece'''

    return [10**prob for prob, _, _ in kenlm_model.full_scores(remove_punc(s), bos=False, eos=False)]

get_score("আমার নাম শুভাশীষ")

get_score("শুভাশীষ কলকাতায় থাকে")

list(kenlm_model.full_scores("আমার নাম শুভাশীষ", bos=False, eos=False))

def prior_prob(word):   # SMOOTHED PRIOR PROBABILITY
    return 10**kenlm_model.score(remove_punc(word), bos=False, eos=False)

# testing
cds = ["পূর্বাভাস", "পূর্বাভাব", "পূর্বাভাগ", "পূর্বাশা", "পূর্ব"]
for c in cds:
    print(prior_prob(c))

"""## 5. Channel Probability"""

# READ ALL SENTENCES
with open(bn_sentences_path, "r") as file:
    sentences = file.read().split("\n")

# TEST
sentences[:5]

# COUNT MATRIX
# DEPICTED
# def get_count(lexem):
#     unigram_count = dict()
#     bigram_count = dict()
#     for sentence in sentences:
#         bigrams = ["".join(a) for a in list(ngrams(lexem, n=2))]
#         unigrams = ["".join(a) for a in list(ngrams(lexem, n=1))]
#         bcount = Counter(bigrams)
#         ucount = Counter(unigrams)
#
#         for key in bcount:  # Count all bigrams
#             try:
#                 bigram_count[key] += bcount[key]
#             except:
#                 bigram_count[key] = bcount[key]
#
#         for key in ucount:  # count all unigrams
#             try:
#                 unigram_count[key] += ucount[key]
#             except:
#                 unigram_count[key] = ucount[key]
#
#     return unigram_count, bigram_count

def get_count(lexem):
    '''----
Description: returns character wise unigram and bigram frequency of the given lexem
Args:   lexem (str)     = word whose uni and bi -gram frequency are to be calculated
Return: (dictionary)    = character unigram frequency as a key value pair
        (dictionary)    = character bigram frequency as a key value pair '''

    u = list(lexem)
    b = list(ngrams(lexem, n=2))
    unigram_counts = dict()
    temp1 = [get_uni_letter_count(i) for i in u]
    temp2 = [get_bi_letter_count(i,j) for (i,j) in b]
    return dict(zip(u, temp1)), dict(zip(b, temp2))

get_count("কলিকাতা")

# CHAR PROBAIBLITY
def char_probability(x, y, type):
    if x == "#" or x == "":
        xi = -1
    else:
        xi = char_lookup[x]

    if y == "#" or y == "":
        yi = -1
    else:
        yi = char_lookup[y]

    # print(f"{type}({xi},{yi})")
    uni_count, bi_count = get_count(tuple(x+y))
    if type.lower() == "d":
        return (int(bn_del[xi][yi])+1)/(int(bi_count[tuple(x+y)])+7056)
    elif type.lower() == "i":
        return (int(bn_ins[yi][xi])+1)/(int(uni_count[y])+84)
    elif type.lower() == "s":
        return (int(bn_sub[xi][yi])+1)/(int(uni_count[y])+84)
    return -1

char_probability("ক","ত","s")

# CHANNEL PROBABILITY
def get_channel_probability(word, candidate):
    edit_sequence = get_edit_sequence(word, candidate)
    probability = 0
    for sequence in edit_sequence:
        if sequence != '':
            sequence_list = list(sequence)
            probability += char_probability(sequence_list[2], sequence_list[1], sequence_list[0])
    return probability

# TEST
s = "কলকাতা"
d = "কলিকাতা"
get_channel_probability(s,d)

"""## Testing"""

find_valid_candidates("জন", 1)

# TEST
prior, channel, prob = {}, {}, {}
typo = "পারি"
for candidate in find_valid_candidates(typo, 1):
    a = prior_prob(candidate)
    b = get_channel_probability(typo, candidate)
    prior[candidate] = a
    channel[f"[{typo},{candidate}]"] = b
    prob[f"[{typo},{candidate}]"] = a*b

prior = sorted(prior.items(), key=lambda x: x[1], reverse=True)
channel = sorted(channel.items(), key=lambda x: x[1], reverse=True)
prob = sorted(prob.items(), key=lambda x: x[1], reverse=True)

for key in prior:
    print(key)

for key in channel:
    print(key)

for p in prob:
    print(p)

"""## 6. Final Estimation (real word)"""

# MAKE N-GRAM
def extract_ngrams(sentence, n):
    n_grams = ngrams(nltk.word_tokenize(sentence), n)
    # return [ ' '.join(grams) for grams in n_grams]
    return [grams for grams in n_grams]

# TEST
sent = "কলিকাতা সহ দক্ষিণবঙ্গের সব জেলাতেই বৃষ্টির সঙ্গে থাকবে দমকা ঝোড়ো হাওয়া"
extract_ngrams(sent, 2)

def get_lm_prob(sent):
    return 10**kenlm_model.score(remove_punc(sent))

get_lm_prob("সঙ্গে থাকবে")

get_lm_prob("আমার নাম শুভাশীষ")

get_lm_prob("শুভাশীষ কলকাতায় থাকে")

"""## Real Word"""

# GENERATE ALL CANDIDATES
def candidate_generator(sent, med):
    '''----
Description: generate all candidates of all tokens form real word spelling correction
Args:   sent (str)  = given sentence to check
        med (int)   = minimum edit distance to check for candidates
Return: (2D list)   = list of candidate words according to their position'''

    candidates = []
    for word in remove_punc(sent).strip().split(" "):   # iterate through each words
        candidates.append(find_valid_candidates(word, med))
    return candidates

alpha = 0.95 # config

# GENERATE ALL SENTENCE PROBABILITY
def sentence_probability(sent, med):
    '''----
Description: generate candidate sentences and determine its probabilities using ngram LM
Args:   sent (string)   = missplled sentence
        med (int)       = minimum edit distance in which the candidates need to be count
Return: sent_prob (dictionary)  = candidate sentence and its probability as dictionary
        log (list)      = log messages during execution (for debugging) '''

    candidates_list = candidate_generator(sent, med)
    sent_prob = {}
    log = list()

    for i in range(len(candidates_list)):
        for candidate in candidates_list[i]:
            edit_prob = alpha   # when P(w|w)
            lm_prob = 1
            candidate_sentence = " ".join(sent.split()[:i]+[candidate]+sent.split()[i+1:])

            log.append("--"*30)                 # log
            # print("= "*30)                    # print
            log.append(candidate_sentence)      # log
            # print(candidate_sentence)         # print
            noisy_channel_prob = 0

            if sent.split()[i] != candidate:
                edit_prob = get_channel_probability(sent.split()[i], candidate)
                pr_prob = prior_prob(candidate)
                noisy_channel_prob = edit_prob * pr_prob
                log.append(f"P({sent.split()[i]}|{candidate}): {noisy_channel_prob}")        # log
                # print(f"P({sent.split()[i]}|{candidate}): {edit_prob}")           # print

            for ngram in extract_ngrams(candidate_sentence, 2):
                lm_prob *= get_lm_prob(" ".join(ngram))
                log.append(f"{ngram}: {get_lm_prob(' '.join(ngram))}")              # log
                # print(f"{ngram}: {get_lm_prob(' '.join(ngram))}")                 # print
            sent_prob[candidate_sentence] = (noisy_channel_prob * lm_prob)

    return sent_prob, log

prob = sentence_probability("তিনি পুলিশের কথা শুনতে পানি", 1)[0]
sorted(prob.items(), key=lambda x: x[1], reverse=True)[0]

prob = sentence_probability("তিনি পুলিশের কথা শুনতে পান", 1)[0]
sorted(prob.items(), key=lambda x: x[1], reverse=True)[0]

def get_correct_sent(sent, med, filename="log.txt"):
    '''----
Description: take a errornious sentence as argument and predict its correct output
Args:   sent (string)   = errornious sentence
        med (int)       = minimum edit distance within which candidates are to be generated
        filename (string) = path to log file (optional)
Return: (string)    = predicted sentence '''

    prob, log = sentence_probability(sent, med)

    with open(filename, "w") as file:
        for line in log:
            file.write(line)
            file.write("\n")

    with open(filename, "w") as file:
        for a in log:
            file.write(str(a))
            file.write("\n")

    return sorted(prob.items(), key=lambda x: x[1], reverse=True)[0][0]

# CANDIDATE WORDS STATUS
sent = "বৃষ্টির পূর্বাভা আছে"
prob = get_correct_sent(sent, 1)

print(prob)

sent = "বাংলা একটি সুন্দর ভাসা"
print(get_correct_sent(sent, 1))

sent = "সবশেষে বলত পারি"
print(get_correct_sent(sent, 1))

"""## 7. Testing"""

correction = []
with open("35.txt", "r") as file:
    lines = file.read().split("\n")
    for line in lines[:-1]:
        pred = get_correct_sent(line, 1)
        print(f"Line\t: {line}")
        print(f"Pred\t: {pred}")
        print('-'*50)
        correction.append(pred)

with open("gen.txt", "w") as file:
    for line in correction:
        file.write(f"{line}\n")

"""## Stack Decoding Algorithm"""

stack_size = 3

def get_sequence(prev, word):
    '''
    Args:
        prev (str) = previous sentence
        word (str) = word whose candidate need to be generated
    Return:
        (list) = list of candidate words sorted in descending order
    '''
    candidates = find_valid_candidates(word, 1)     # find candidate in 1 edit distance
    candidates += word                              # add the word itself
    res = dict()
    for candidate in candidates:
        edit_prob = 0.95
        lm_prob = 1
        candidate_sentence = prev+" "+candidate
        noisy_channel_prob = 0
        if candidate != word:
            edit_prob = get_channel_probability(word, candidate)
        pr_prob = prior_prob(candidate)
        noisy_channel_prob = edit_prob * pr_prob
        for ngram in extract_ngrams(candidate_sentence.strip(), 2):
            lm_prob *= get_lm_prob(" ".join(ngram))
        res[candidate_sentence.strip()] = (noisy_channel_prob * lm_prob)

    res = sorted(res.items(), key=lambda x: x[1], reverse=True)
    # print(res)
    # [a.strip() for a,_ in res]
    return dict(res)

# testing
get_sequence("তিনি পুলিশের কথা শুনতে", "পানি")

get_sequence("", "তিনি")

def get_correction(sentence):
    tokens = sentence.split(" ")    # all tokens
    sequences = None
    for i in range(len(tokens)):
        if i == 0:
            sequences = dict(list(get_sequence("", tokens[i]).items())[:stack_size])
        else:
            temp = dict()
            for seq, _ in sequences.items():
                temp.update(get_sequence(seq, tokens[i]))
            temp = sorted(temp.items(), key=lambda x: x[1], reverse=True)[:stack_size]
            sequences = dict(temp)
        print(sequences)
        print("--"*20)

# testing
words = ["তিনি পুলশের কথা শুনতে পানি"]
for word in words:
    get_correction(word)
    print("=="*50)
    print()